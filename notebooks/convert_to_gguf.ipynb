{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LeLM: Merge LoRA + Convert to GGUF + Upload to HuggingFace\n",
        "\n",
        "This notebook:\n",
        "1. Downloads the Qwen3-8B base model + LeLM LoRA adapter\n",
        "2. Merges the LoRA weights into the base model\n",
        "3. Converts to GGUF format (Q4_K_M quantization)\n",
        "4. Uploads the GGUF to HuggingFace\n",
        "\n",
        "**Requirements:** Free Colab GPU runtime (T4 is fine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q torch transformers peft accelerate huggingface_hub sentencepiece\n",
        "!pip install -q llama-cpp-python\n",
        "\n",
        "# Clone llama.cpp for the convert script\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp /content/llama.cpp\n",
        "!pip install -q -r /content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Login to HuggingFace\n",
        "from huggingface_hub import login\n",
        "login()  # Enter your HF token with write access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Load base model + merge LoRA adapter\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen3-8B\"\n",
        "ADAPTER_REPO = \"KenWu/LeLM\"\n",
        "MERGED_PATH = \"/content/lelm-merged\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_REPO)\n",
        "\n",
        "print(f\"Loading base model {BASE_MODEL}...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"cpu\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "print(\"Merging LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_REPO)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\"Saving merged model to {MERGED_PATH}...\")\n",
        "model.save_pretrained(MERGED_PATH, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_PATH)\n",
        "\n",
        "# Free memory\n",
        "del model, base_model\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(\"Merge complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Convert to GGUF (fp16 first, then quantize)\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py \\\n",
        "    /content/lelm-merged \\\n",
        "    --outfile /content/lelm-f16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "print(\"\\nF16 GGUF created!\")\n",
        "!ls -lh /content/lelm-f16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Quantize to Q4_K_M (best quality/size tradeoff, ~5GB)\n",
        "# First build llama-quantize\n",
        "!cd /content/llama.cpp && cmake -B build && cmake --build build --target llama-quantize -j$(nproc)\n",
        "\n",
        "!/content/llama.cpp/build/bin/llama-quantize \\\n",
        "    /content/lelm-f16.gguf \\\n",
        "    /content/LeLM-Q4_K_M.gguf \\\n",
        "    Q4_K_M\n",
        "\n",
        "print(\"\\nQuantized GGUF created!\")\n",
        "!ls -lh /content/LeLM-Q4_K_M.gguf\n",
        "\n",
        "# Also create Q8_0 for higher quality\n",
        "!/content/llama.cpp/build/bin/llama-quantize \\\n",
        "    /content/lelm-f16.gguf \\\n",
        "    /content/LeLM-Q8_0.gguf \\\n",
        "    Q8_0\n",
        "\n",
        "print(\"\\nQ8_0 GGUF created!\")\n",
        "!ls -lh /content/LeLM-Q8_0.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Upload GGUFs to HuggingFace\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "REPO_ID = \"KenWu/LeLM-GGUF\"\n",
        "\n",
        "# Create the repo\n",
        "api.create_repo(REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Upload Q4_K_M\n",
        "print(\"Uploading Q4_K_M...\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/LeLM-Q4_K_M.gguf\",\n",
        "    path_in_repo=\"LeLM-Q4_K_M.gguf\",\n",
        "    repo_id=REPO_ID,\n",
        ")\n",
        "\n",
        "# Upload Q8_0\n",
        "print(\"Uploading Q8_0...\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/LeLM-Q8_0.gguf\",\n",
        "    path_in_repo=\"LeLM-Q8_0.gguf\",\n",
        "    repo_id=REPO_ID,\n",
        ")\n",
        "\n",
        "print(f\"\\nDone! View at: https://huggingface.co/{REPO_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Upload README for the GGUF repo\n",
        "readme = \"\"\"---\n",
        "base_model: Qwen/Qwen3-8B\n",
        "license: apache-2.0\n",
        "tags:\n",
        "  - gguf\n",
        "  - lora-merged\n",
        "  - nba\n",
        "  - sports-analysis\n",
        "  - qwen3\n",
        "pipeline_tag: text-generation\n",
        "quantized_by: llama.cpp\n",
        "---\n",
        "\n",
        "# LeLM-GGUF\n",
        "\n",
        "GGUF quantizations of [KenWu/LeLM](https://huggingface.co/KenWu/LeLM), an NBA take analysis model fine-tuned on Qwen3-8B.\n",
        "\n",
        "## Available Quantizations\n",
        "\n",
        "| File | Quant | Size | Description |\n",
        "|---|---|---|---|\n",
        "| `LeLM-Q4_K_M.gguf` | Q4_K_M | ~5 GB | Best balance of quality and size |\n",
        "| `LeLM-Q8_0.gguf` | Q8_0 | ~8.5 GB | Higher quality, larger |\n",
        "\n",
        "## Usage with Ollama\n",
        "\n",
        "Create a `Modelfile`:\n",
        "\n",
        "```\n",
        "FROM ./LeLM-Q4_K_M.gguf\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "\n",
        "SYSTEM You are LeLM, an expert NBA analyst. Fact-check basketball takes using real statistics. Be direct, witty, and back everything with numbers.\n",
        "```\n",
        "\n",
        "Then run:\n",
        "```bash\n",
        "ollama create lelm -f Modelfile\n",
        "ollama run lelm \"Fact check: LeBron is washed\"\n",
        "```\n",
        "\n",
        "## Usage with llama.cpp\n",
        "\n",
        "```bash\n",
        "llama-cli -m LeLM-Q4_K_M.gguf -p \"Fact check this NBA take: Steph Curry is the GOAT\" -n 512\n",
        "```\n",
        "\n",
        "## Part of LeGM-Lab\n",
        "\n",
        "This model powers [LeGM-Lab](https://github.com/KenWuqianghao/LeGM-Lab), an LLM-powered NBA take analysis and roasting bot.\n",
        "\"\"\"\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=readme.encode(),\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=REPO_ID,\n",
        ")\n",
        "print(f\"README uploaded! https://huggingface.co/{REPO_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Test (Optional)\n",
        "\n",
        "Test the merged model before uploading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Quick test of the merged model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/lelm-merged\",\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/lelm-merged\")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Fact check this NBA take: LeBron is washed\"}]\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(inputs, max_new_tokens=256)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ]
}