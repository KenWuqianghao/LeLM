{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeLM â€” Fine-tuned LLM for NBA Hot Takes\n",
    "\n",
    "**Prerequisites**: Upload `train.jsonl` and `val.jsonl` to `Google Drive > MyDrive > LeLM/`\n",
    "\n",
    "**Runtime**: GPU > T4 (free) for Qwen3-8B, or A100 (Pro) for Qwen3-14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Train: 2434 examples\n",
      "Val:   129 examples\n",
      "GPU: Tesla T4 (15.6 GB)\n",
      "Model: unsloth/Qwen3-8B-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install + Mount Drive + Config\n",
    "!pip install -q unsloth transformers trl datasets peft bitsandbytes\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LeLM'\n",
    "TRAIN_FILE = Path(DRIVE_DIR) / 'train.jsonl'\n",
    "VAL_FILE = Path(DRIVE_DIR) / 'val.jsonl'\n",
    "OUTPUT_DIR = os.path.join(DRIVE_DIR, 'lelm-adapter')\n",
    "\n",
    "# Verify data exists\n",
    "assert TRAIN_FILE.exists(), f'Missing {TRAIN_FILE} â€” upload train.jsonl to Drive/LeLM/'\n",
    "assert VAL_FILE.exists(), f'Missing {VAL_FILE} â€” upload val.jsonl to Drive/LeLM/'\n",
    "print(f'Train: {sum(1 for _ in open(TRAIN_FILE))} examples')\n",
    "print(f'Val:   {sum(1 for _ in open(VAL_FILE))} examples')\n",
    "\n",
    "# Auto-detect GPU and pick model\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'GPU: {gpu_name} ({vram_gb:.1f} GB)')\n",
    "\n",
    "if 'A100' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-14B-bnb-4bit'\n",
    "elif 'T4' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-8B-bnb-4bit'\n",
    "else:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-4B-bnb-4bit'\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    'You are an unapologetically bold NBA analyst who lives for hot takes. '\n",
    "    'You speak with absolute conviction, back up your claims with stats and game knowledge, '\n",
    "    \"but aren't afraid to be controversial. You have strong opinions on player legacies, \"\n",
    "    'team strategies, and playoff predictions. Your style is passionate, entertaining, '\n",
    "    \"and occasionally provocative â€” like a mix of Skip Bayless's confidence, Charles Barkley's \"\n",
    "    \"humor, and Zach Lowe's basketball IQ. Never hedge. Never be boring. Every take should \"\n",
    "    'make someone want to argue with you.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.2.1: Fast Qwen3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.35. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load model + LoRA\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing='unsloth',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286e5ebc77ef4a9e874292b2821adf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f277d38b128242fe8b189917a0435c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2434 | Val: 129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1411ee585249454f8ca28ce3b6b5c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/2434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21b461fbc7445aea72adfd081f2580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,434 | Num Epochs = 3 | Total steps = 915\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 174,587,904 of 8,365,323,264 (2.09% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [915/915 1:35:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.839827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.754870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.803928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to /content/drive/MyDrive/LeLM/lelm-adapter\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "train_dataset = Dataset.from_list([json.loads(l) for l in open(TRAIN_FILE)])\n",
    "val_dataset = Dataset.from_list([json.loads(l) for l in open(VAL_FILE)])\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples['messages']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {'text': texts}\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)}')\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_steps=10,\n",
    "        optim='adamw_8bit',\n",
    "        weight_decay=0.01,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        save_strategy='epoch',\n",
    "        eval_strategy='epoch',\n",
    "        seed=42,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dataset_text_field='text',\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter to Drive\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f'Adapter saved to {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Give me your hottest LeBron James take.\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "You want to talk about clutch gene? LeBron's .414 career 3PT percentage in the 4th quarter puts him in a league of his own. Other guys may have higher totals, but none of them can match the consistency and volume of LeBron's clutch threes.\n",
      "\n",
      "\n",
      ">> Is Nikola Jokic the best player in the NBA right now?\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "I love how some people still can't comprehend that a 7ft0 player can dominate the glass like this. Jokic's got the size, strength, and technique to outwork anyone on the court, it's a beautiful thing to watch\n",
      "\n",
      "\n",
      ">> Who's the most overrated player in the league?\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The most overrated player in the league today is Karl Anthony Towns. I donâ€™t think people realize how mediocre heâ€™s been for a few years now. The only good thing you can say about him is he can post up and put up points, but he has a terrible assist rate and his defense is garbage.\n",
      "\n",
      "\n",
      ">> Give me your boldest Finals prediction.\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "I feel like people on this sub actually watch NBA games\n",
      "\n",
      "\n",
      ">> Is the 3-point revolution ruining basketball?\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "I think it's more that the 3 ball has always been a part of the game. It's just more emphasized now because it's easier to score from there, and more valuable, so players are doing it more.\n",
      "\n",
      "\n",
      ">> Give me your hottest Kevin Durant take.\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "KD's Achilles injury was a defining moment of his career. Instead of being broken, he became more dominant than ever. The dude dropped 51 on the Warriors in his first game back, and I'm still getting chills thinking about it. KD is one of the most resilient players I've ever seen.\n",
      "\n",
      "\n",
      ">> Is KD the best scorer in NBA history?\n",
      "----------------------------------------\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Durant's midrange efficiency is a game-changer. He's got the perfect blend of technique and touch, with a 45% midrange clip that puts him in elite company. You can't teach that level of consistency, it's all about feel and execution. KD delivers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt, max_new_tokens=512):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=max_new_tokens,\n",
    "        temperature=0.8, top_p=0.9, do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "demos = [\n",
    "    'Give me your hottest LeBron James take.',\n",
    "    'Is Nikola Jokic the best player in the NBA right now?',\n",
    "    \"Who's the most overrated player in the league?\",\n",
    "    'Give me your boldest Finals prediction.',\n",
    "    'Is the 3-point revolution ruining basketball?',\n",
    "    'Give me your hottest Kevin Durant take.',\n",
    "    'Is KD the best scorer in NBA history?',\n",
    "]\n",
    "\n",
    "for prompt in demos:\n",
    "    print(f'\\n>> {prompt}')\n",
    "    print('-' * 40)\n",
    "    print(generate(prompt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "KD's clutch reputation is 100% earned. He's got the highest career PPG among any player with 1000+ games played, and 4 of his 10 scoring titles. When the chips are on the table, he goes above and beyond.\n"
     ]
    }
   ],
   "source": [
    "# Interactive â€” type your own prompts\n",
    "prompt = 'Was KD the better player than Steph when he was on the Warriors?'  # @param {type:\"string\"}\n",
    "print(generate(prompt))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
