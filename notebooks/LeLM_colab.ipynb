{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeLM — Fine-tuned LLM for NBA Hot Takes\n",
    "\n",
    "**Prerequisites**: Upload `train.jsonl` and `val.jsonl` to `Google Drive > MyDrive > LeLM/`\n",
    "\n",
    "**Runtime**: GPU > T4 (free) for Qwen3-8B, or A100 (Pro) for Qwen3-14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Train: 2434 examples\n",
      "Val:   129 examples\n",
      "GPU: Tesla T4 (15.6 GB)\n",
      "Model: unsloth/Qwen3-8B-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install + Mount Drive + Config\n",
    "!pip install -q unsloth transformers trl datasets peft bitsandbytes\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LeLM'\n",
    "TRAIN_FILE = Path(DRIVE_DIR) / 'train.jsonl'\n",
    "VAL_FILE = Path(DRIVE_DIR) / 'val.jsonl'\n",
    "OUTPUT_DIR = os.path.join(DRIVE_DIR, 'lelm-adapter')\n",
    "\n",
    "# Verify data exists\n",
    "assert TRAIN_FILE.exists(), f'Missing {TRAIN_FILE} — upload train.jsonl to Drive/LeLM/'\n",
    "assert VAL_FILE.exists(), f'Missing {VAL_FILE} — upload val.jsonl to Drive/LeLM/'\n",
    "print(f'Train: {sum(1 for _ in open(TRAIN_FILE))} examples')\n",
    "print(f'Val:   {sum(1 for _ in open(VAL_FILE))} examples')\n",
    "\n",
    "# Auto-detect GPU and pick model\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'GPU: {gpu_name} ({vram_gb:.1f} GB)')\n",
    "\n",
    "if 'A100' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-14B-bnb-4bit'\n",
    "elif 'T4' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-8B-bnb-4bit'\n",
    "else:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-4B-bnb-4bit'\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    'You are an unapologetically bold NBA analyst who lives for hot takes. '\n",
    "    'You speak with absolute conviction, back up your claims with stats and game knowledge, '\n",
    "    \"but aren't afraid to be controversial. You have strong opinions on player legacies, \"\n",
    "    'team strategies, and playoff predictions. Your style is passionate, entertaining, '\n",
    "    \"and occasionally provocative — like a mix of Skip Bayless's confidence, Charles Barkley's \"\n",
    "    \"humor, and Zach Lowe's basketball IQ. Never hedge. Never be boring. Every take should \"\n",
    "    'make someone want to argue with you.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.2.1: Fast Qwen3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.35. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load model + LoRA\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing='unsloth',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Train\nfrom datasets import Dataset\nfrom trl import SFTTrainer, SFTConfig\n\ntrain_dataset = Dataset.from_list([json.loads(l) for l in open(TRAIN_FILE)])\nval_dataset = Dataset.from_list([json.loads(l) for l in open(VAL_FILE)])\nprint(f'Train: {len(train_dataset)} | Val: {len(val_dataset)}')\n\ndef formatting_func(example):\n    return [tokenizer.apply_chat_template(\n        example['messages'], tokenize=False, add_generation_prompt=False\n    )]\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=SFTConfig(\n        output_dir=OUTPUT_DIR,\n        num_train_epochs=3,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        lr_scheduler_type='cosine',\n        warmup_steps=10,\n        optim='adamw_8bit',\n        weight_decay=0.01,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,\n        save_strategy='epoch',\n        eval_strategy='epoch',\n        seed=42,\n        max_seq_length=MAX_SEQ_LENGTH,\n        dataset_text_field='text',\n    ),\n    formatting_func=formatting_func,\n)\n\ntrainer.train()\n\n# Save adapter to Drive\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f'Adapter saved to {OUTPUT_DIR}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt, max_new_tokens=512):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=max_new_tokens,\n",
    "        temperature=0.8, top_p=0.9, do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "demos = [\n",
    "    'Give me your hottest LeBron James take.',\n",
    "    'Is Nikola Jokic the best player in the NBA right now?',\n",
    "    \"Who's the most overrated player in the league?\",\n",
    "    'Give me your boldest Finals prediction.',\n",
    "    'Is the 3-point revolution ruining basketball?',\n",
    "    'Give me your hottest Kevin Durant take.',\n",
    "    'Is KD the best scorer in NBA history?',\n",
    "]\n",
    "\n",
    "for prompt in demos:\n",
    "    print(f'\\n>> {prompt}')\n",
    "    print('-' * 40)\n",
    "    print(generate(prompt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive — type your own prompts\n",
    "prompt = 'Was KD the better player than Steph when he was on the Warriors?'  # @param {type:\"string\"}\n",
    "print(generate(prompt))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}