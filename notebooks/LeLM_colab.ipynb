{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeLM — Fine-tuned LLM for NBA Hot Takes\n",
    "\n",
    "**Prerequisites**: Upload `train.jsonl` and `val.jsonl` to `Google Drive > MyDrive > LeLM/`\n",
    "\n",
    "**Runtime**: GPU > T4 (free) for Qwen3-8B, or A100 (Pro) for Qwen3-14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install + Mount Drive + Config\n",
    "!pip install -q unsloth transformers trl datasets peft bitsandbytes\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LeLM'\n",
    "TRAIN_FILE = Path(DRIVE_DIR) / 'train.jsonl'\n",
    "VAL_FILE = Path(DRIVE_DIR) / 'val.jsonl'\n",
    "OUTPUT_DIR = os.path.join(DRIVE_DIR, 'lelm-adapter')\n",
    "\n",
    "# Verify data exists\n",
    "assert TRAIN_FILE.exists(), f'Missing {TRAIN_FILE} — upload train.jsonl to Drive/LeLM/'\n",
    "assert VAL_FILE.exists(), f'Missing {VAL_FILE} — upload val.jsonl to Drive/LeLM/'\n",
    "print(f'Train: {sum(1 for _ in open(TRAIN_FILE))} examples')\n",
    "print(f'Val:   {sum(1 for _ in open(VAL_FILE))} examples')\n",
    "\n",
    "# Auto-detect GPU and pick model\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'GPU: {gpu_name} ({vram_gb:.1f} GB)')\n",
    "\n",
    "if 'A100' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-14B-bnb-4bit'\n",
    "elif 'T4' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-8B-bnb-4bit'\n",
    "else:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-4B-bnb-4bit'\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    'You are an unapologetically bold NBA analyst who lives for hot takes. '\n",
    "    'You speak with absolute conviction, back up your claims with stats and game knowledge, '\n",
    "    \"but aren't afraid to be controversial. You have strong opinions on player legacies, \"\n",
    "    'team strategies, and playoff predictions. Your style is passionate, entertaining, '\n",
    "    \"and occasionally provocative — like a mix of Skip Bayless's confidence, Charles Barkley's \"\n",
    "    \"humor, and Zach Lowe's basketball IQ. Never hedge. Never be boring. Every take should \"\n",
    "    'make someone want to argue with you.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load model + LoRA\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing='unsloth',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "train_dataset = Dataset.from_list([json.loads(l) for l in open(TRAIN_FILE)])\n",
    "val_dataset = Dataset.from_list([json.loads(l) for l in open(VAL_FILE)])\n",
    "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)}')\n",
    "\n",
    "def formatting_func(example):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example['messages'], tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_steps=10,\n",
    "        optim='adamw_8bit',\n",
    "        weight_decay=0.01,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        save_strategy='epoch',\n",
    "        eval_strategy='epoch',\n",
    "        seed=42,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dataset_text_field='text',\n",
    "    ),\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter to Drive\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f'Adapter saved to {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt, max_new_tokens=512):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=max_new_tokens,\n",
    "        temperature=0.8, top_p=0.9, do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "demos = [\n",
    "    'Give me your hottest LeBron James take.',\n",
    "    'Is Nikola Jokic the best player in the NBA right now?',\n",
    "    \"Who's the most overrated player in the league?\",\n",
    "    'Give me your boldest Finals prediction.',\n",
    "    'Is the 3-point revolution ruining basketball?',\n",
    "    'Give me your hottest Kevin Durant take.',\n",
    "    'Is KD the best scorer in NBA history?',\n",
    "]\n",
    "\n",
    "for prompt in demos:\n",
    "    print(f'\\n>> {prompt}')\n",
    "    print('-' * 40)\n",
    "    print(generate(prompt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive — type your own prompts\n",
    "prompt = 'Was KD the better player than Steph when he was on the Warriors?'  # @param {type:\"string\"}\n",
    "print(generate(prompt))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
