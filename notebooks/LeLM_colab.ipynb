{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeLM — Fine-tuned LLM for NBA Hot Takes\n",
    "\n",
    "All-in-one Colab notebook for scraping, processing, training, and inference.\n",
    "\n",
    "**Runtime**: T4 GPU (free tier) for Qwen3-8B, or A100 (Pro) for Qwen3-14B.\n",
    "\n",
    "**Usage**: Run cells top to bottom. Mount Google Drive to persist checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.9/181.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q requests unsloth transformers trl datasets peft bitsandbytes pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive for persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LeLM'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: CPU\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-486/1893735251.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgpu_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'CPU'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'GPU: {gpu_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'A100'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpu_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m             )\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import torch\n",
    "\n",
    "# Choose model based on available GPU\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print(f'GPU: {gpu_name}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "if 'A100' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-14B-bnb-4bit'\n",
    "    print('Using Qwen3-14B (A100 detected)')\n",
    "elif 'T4' in gpu_name:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-8B-bnb-4bit'\n",
    "    print('Using Qwen3-8B (T4 detected)')\n",
    "else:\n",
    "    MODEL_NAME = 'unsloth/Qwen3-4B-bnb-4bit'\n",
    "    print(f'Using Qwen3-4B (fallback for {gpu_name})')\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an unapologetically bold NBA analyst who lives for hot takes. \"\n",
    "    \"You speak with absolute conviction, back up your claims with stats and game knowledge, \"\n",
    "    \"but aren't afraid to be controversial. You have strong opinions on player legacies, \"\n",
    "    \"team strategies, and playoff predictions. Your style is passionate, entertaining, \"\n",
    "    \"and occasionally provocative — like a mix of Skip Bayless's confidence, Charles Barkley's \"\n",
    "    \"humor, and Zach Lowe's basketball IQ. Never hedge. Never be boring. Every take should \"\n",
    "    \"make someone want to argue with you.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape Reddit Data\n",
    "\n",
    "Uses Reddit's public JSON endpoints — no API key needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import praw\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_FILE = Path(DRIVE_DIR) / 'reddit_posts.jsonl'\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ['REDDIT_CLIENT_ID'],\n",
    "    client_secret=os.environ['REDDIT_CLIENT_SECRET'],\n",
    "    user_agent=os.environ['REDDIT_USER_AGENT'],\n",
    ")\n",
    "\n",
    "subreddit = reddit.subreddit('nba')\n",
    "scraped_ids = set()\n",
    "\n",
    "QUERIES = [\n",
    "    'hot take', 'unpopular opinion', 'overrated underrated',\n",
    "    'bold prediction', 'worst take', 'GOAT debate',\n",
    "]\n",
    "\n",
    "with open(RAW_FILE, 'a') as f:\n",
    "    for query in QUERIES:\n",
    "        print(f'Searching: {query}...')\n",
    "        for post in subreddit.search(query, sort='top', limit=500):\n",
    "            if post.id in scraped_ids:\n",
    "                continue\n",
    "            f.write(json.dumps({\n",
    "                'id': post.id, 'type': 'post',\n",
    "                'title': post.title, 'selftext': post.selftext,\n",
    "                'score': post.score, 'created_utc': post.created_utc,\n",
    "            }) + '\\n')\n",
    "            scraped_ids.add(post.id)\n",
    "\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for c in sorted(post.comments.list(), key=lambda c: c.score, reverse=True)[:10]:\n",
    "                if c.id not in scraped_ids:\n",
    "                    f.write(json.dumps({\n",
    "                        'id': c.id, 'type': 'comment',\n",
    "                        'post_title': post.title, 'body': c.body,\n",
    "                        'score': c.score, 'created_utc': c.created_utc,\n",
    "                    }) + '\\n')\n",
    "                    scraped_ids.add(c.id)\n",
    "        time.sleep(2)\n",
    "\n",
    "print(f'Total scraped: {len(scraped_ids)} items -> {RAW_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_FILE = Path(DRIVE_DIR) / 'reddit_posts.jsonl'\n",
    "HEADERS = {'User-Agent': 'LeLM-scraper/1.0'}\n",
    "\n",
    "QUERIES = [\n",
    "    'hot take', 'unpopular opinion', 'overrated underrated',\n",
    "    'bold prediction', 'worst take', 'GOAT debate',\n",
    "]\n",
    "\n",
    "def fetch_json(url, params=None):\n",
    "    for attempt in range(3):\n",
    "        resp = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "        if resp.status_code == 429:\n",
    "            wait = int(resp.headers.get('Retry-After', 10))\n",
    "            print(f'  Rate limited, waiting {wait}s...')\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    raise RuntimeError(f'Failed after retries: {url}')\n",
    "\n",
    "scraped_ids = set()\n",
    "\n",
    "with open(RAW_FILE, 'a') as f:\n",
    "    for query in QUERIES:\n",
    "        print(f'Searching: {query}...')\n",
    "        after = None\n",
    "        for page in range(5):\n",
    "            params = {'q': query, 'restrict_sr': 'on', 'sort': 'top', 't': 'all', 'limit': 100}\n",
    "            if after:\n",
    "                params['after'] = after\n",
    "            data = fetch_json('https://www.reddit.com/r/nba/search.json', params)\n",
    "            posts = data['data']['children']\n",
    "            if not posts:\n",
    "                break\n",
    "\n",
    "            for pw in posts:\n",
    "                if pw['kind'] != 't3':\n",
    "                    continue\n",
    "                d = pw['data']\n",
    "                if d['id'] in scraped_ids:\n",
    "                    continue\n",
    "                f.write(json.dumps({\n",
    "                    'id': d['id'], 'type': 'post',\n",
    "                    'title': d.get('title', ''), 'selftext': d.get('selftext', ''),\n",
    "                    'score': d.get('score', 0), 'created_utc': d.get('created_utc', 0),\n",
    "                }) + '\\n')\n",
    "                scraped_ids.add(d['id'])\n",
    "\n",
    "                # Fetch top comments\n",
    "                time.sleep(2)\n",
    "                try:\n",
    "                    cdata = fetch_json(f\"https://www.reddit.com/r/nba/comments/{d['id']}.json\",\n",
    "                                       {'sort': 'top', 'limit': 10})\n",
    "                    if len(cdata) >= 2:\n",
    "                        for child in cdata[1]['data'].get('children', [])[:10]:\n",
    "                            if child['kind'] != 't1':\n",
    "                                continue\n",
    "                            cd = child['data']\n",
    "                            if cd['id'] not in scraped_ids:\n",
    "                                f.write(json.dumps({\n",
    "                                    'id': cd['id'], 'type': 'comment',\n",
    "                                    'post_title': d.get('title', ''), 'body': cd.get('body', ''),\n",
    "                                    'score': cd.get('score', 0), 'created_utc': cd.get('created_utc', 0),\n",
    "                                }) + '\\n')\n",
    "                                scraped_ids.add(cd['id'])\n",
    "                except Exception as e:\n",
    "                    print(f'  Comment error: {e}')\n",
    "\n",
    "            after = data['data'].get('after')\n",
    "            if not after:\n",
    "                break\n",
    "            time.sleep(2)\n",
    "\n",
    "print(f'Total scraped: {len(scraped_ids)} items -> {RAW_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "DIRECT_TEMPLATES = [\n",
    "    \"Give me your hottest NBA take right now.\",\n",
    "    \"What's your most controversial NBA opinion?\",\n",
    "    \"Drop an NBA hot take that would get you yelled at on Twitter.\",\n",
    "    \"Give me a spicy NBA take.\",\n",
    "    \"What's your boldest NBA prediction?\",\n",
    "    \"Hit me with an unpopular NBA opinion.\",\n",
    "]\n",
    "\n",
    "TOPIC_TEMPLATES = [\n",
    "    \"What's your hot take on {topic}?\",\n",
    "    \"Give me your most controversial opinion about {topic}.\",\n",
    "    \"Drop a spicy take about {topic}.\",\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'/?(u|r)/\\w+', '', text)\n",
    "    text = re.sub(r'\\[removed\\]|\\[deleted\\]', '', text)\n",
    "    text = re.sub(r'edit:.*$', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def trigram_jaccard(a, b):\n",
    "    def tri(s): return Counter(s.lower()[i:i+3] for i in range(len(s)-2))\n",
    "    ta, tb = tri(a), tri(b)\n",
    "    if not ta or not tb: return 0.0\n",
    "    return sum((ta & tb).values()) / sum((ta | tb).values())\n",
    "\n",
    "# Load raw data\n",
    "items = [json.loads(l) for l in open(RAW_FILE)]\n",
    "print(f'Raw items: {len(items)}')\n",
    "\n",
    "# Filter\n",
    "filtered = []\n",
    "for item in items:\n",
    "    if item['type'] == 'post':\n",
    "        text = clean_text(f\"{item['title']} {item.get('selftext', '')}\")\n",
    "        if item.get('score', 0) < 10: continue\n",
    "    else:\n",
    "        text = clean_text(item.get('body', ''))\n",
    "        if item.get('score', 0) < 25: continue\n",
    "    if len(text) < 50 or len(text) > 1500: continue\n",
    "    if any(m in text.lower() for m in ['i am a bot', 'beep boop']): continue\n",
    "    topic = item.get('title', item.get('post_title', ''))\n",
    "    filtered.append((text, topic))\n",
    "print(f'After filter: {len(filtered)}')\n",
    "\n",
    "# Deduplicate\n",
    "unique = []\n",
    "for text, topic in filtered:\n",
    "    if not any(trigram_jaccard(text, u[0]) > 0.8 for u in unique):\n",
    "        unique.append((text, topic))\n",
    "print(f'After dedup: {len(unique)}')\n",
    "\n",
    "# Format\n",
    "rng = random.Random(42)\n",
    "examples = []\n",
    "for text, topic in unique:\n",
    "    if rng.random() < 0.4 or not topic:\n",
    "        user_msg = rng.choice(DIRECT_TEMPLATES)\n",
    "    else:\n",
    "        user_msg = rng.choice(TOPIC_TEMPLATES).format(topic=topic)\n",
    "    examples.append({'messages': [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': user_msg},\n",
    "        {'role': 'assistant', 'content': text},\n",
    "    ]})\n",
    "rng.shuffle(examples)\n",
    "\n",
    "# Split\n",
    "split = int(len(examples) * 0.95)\n",
    "train_data, val_data = examples[:split], examples[split:]\n",
    "\n",
    "TRAIN_FILE = Path(DRIVE_DIR) / 'train.jsonl'\n",
    "VAL_FILE = Path(DRIVE_DIR) / 'val.jsonl'\n",
    "for path, data in [(TRAIN_FILE, train_data), (VAL_FILE, val_data)]:\n",
    "    with open(path, 'w') as f:\n",
    "        for ex in data: f.write(json.dumps(ex) + '\\n')\n",
    "\n",
    "print(f'Train: {len(train_data)} | Val: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fine-tune with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing='unsloth',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = Dataset.from_list([json.loads(l) for l in open(TRAIN_FILE)])\n",
    "val_dataset = Dataset.from_list([json.loads(l) for l in open(VAL_FILE)])\n",
    "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)}')\n",
    "\n",
    "def formatting_func(example):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example['messages'], tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "OUTPUT_DIR = os.path.join(DRIVE_DIR, 'lelm-adapter')\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_steps=10,\n",
    "        optim='adamw_8bit',\n",
    "        weight_decay=0.01,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        save_strategy='epoch',\n",
    "        eval_strategy='epoch',\n",
    "        seed=42,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dataset_text_field='text',\n",
    "    ),\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f'Adapter saved to {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt, max_new_tokens=512):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=max_new_tokens,\n",
    "        temperature=0.8, top_p=0.9, do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "# Demo prompts\n",
    "demos = [\n",
    "    \"Give me your hottest LeBron James take.\",\n",
    "    \"Is Nikola Jokic the best player in the NBA right now?\",\n",
    "    \"Who's the most overrated player in the league?\",\n",
    "    \"Give me your boldest Finals prediction.\",\n",
    "    \"Is the 3-point revolution ruining basketball?\",\n",
    "]\n",
    "\n",
    "for prompt in demos:\n",
    "    print(f'\\n>> {prompt}')\n",
    "    print('-' * 40)\n",
    "    print(generate(prompt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive — type your own prompts\n",
    "prompt = \"Who will win the championship this year?\"  # @param {type:\"string\"}\n",
    "print(generate(prompt))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
